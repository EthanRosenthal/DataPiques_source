{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "\n",
    "Hey, remember when I wrote those [ungodly]({filename}/2016-01-09-explicit-matrix-factorization-als-sgd.md) [long]({filename}/2016-10-19-implicit-mf-part-1.md) [posts]({filename}/2016-11-7-implicit-mf-part-2.md) about matrix factorization chock-full of gory math? Good news! You can forget it all. We have now entered the Era of Deep Learning, and automatic differentiation shall be our guiding light. \n",
    "\n",
    "<!-- PELICAN_END_SUMMARY -->\n",
    "\n",
    "Less facetiously, *I* have finally spent some time checking out these new-fangled deep learning frameworks, and damn if I am not excited.\n",
    "\n",
    "In this post, I will show you how to use [PyTorch](http://pytorch.org/) to bypass the mess of code from my old post on Explicit Matrix Factorization and instead implement a model that will converge faster in fewer lines of code. \n",
    "\n",
    "But first, let's take a trip down memory lane.\n",
    "\n",
    "## The Dark Ages\n",
    "\n",
    "If you recall from the original matrix factorization post, the key to the derivation was calculus. We defined a loss function which was the mean squared error (MSE) loss between the matrix factorization \"prediction\" and the actual user-item ratings. In order to minimize the loss function, we of course took the derivative and set it equal to zero. \n",
    "\n",
    "For the Stochastic Gradient Descent (SGD) derivation, we iterated through each sample in our dataset and took the derivative of the loss function with respect to each free \"variable\" in our model, which were the user and item latent feature vectors. We then used that derivative to smartly update the values for the latent feature vectors as we surfed down the loss function in search of a minima.\n",
    "\n",
    "I just described what we did in two paragraphs, but it took much longer to actually derive the correct gradient updates and code the whole thing up. The worst part was that, if we wanted to change anything about our prediction model, such as adding regularization or a user bias term, then we had re-derive all of the gradient updates by hand. And if we wanted to play with new techniques, like dropout? Oof, that sounds like a pain to derive!\n",
    "\n",
    "## The Enlightenment\n",
    "\n",
    "The beauty of modern deep learning frameworks is that they utilize automatic differentiation. The phrase means exactly what it sounds like. You simply define your model for prediction, your loss function, and your optimization technique (Vanilla SGD, Adagrad, ADAM, etc...), and the computer will automatically calculate the gradient updates and optimize your model.\n",
    "\n",
    "Additionally, your models can often be easily parallelized across multiple CPU's or turbo-boosted on GPU's with little effort compared to, say, writing Cython code and parallelizing by hand.\n",
    "\n",
    "## Minimum Viable Matrix Factorization\n",
    "\n",
    "We'll walk through the three steps to building a prototype: defining the model, defining the loss, and picking an optimization technique. The latter two steps are largely built into PyTorch, so we'll start with the hardest first.\n",
    "\n",
    "### Model\n",
    "\n",
    "All models in PyTorch subclass from [torch.nn.Module](http://pytorch.org/docs/nn.html#torch.nn.Module), and we will be no different. For our purposes, we only need to define our class and a `forward` method. The `forward` method will simply be our matrix factorization prediction which is the dot product between a user and item latent feature vector.\n",
    "\n",
    "In the language of neural networks, our user and item latent feature vectors are called _embedding_ layers which are analogous to the typical two-dimensional matrices that make up the latent feature vectors. We'll define the embeddings when we initialize the class, and the `forward` method (the prediction) will involve picking out the correct rows of each of the embedding layers and then taking the dot product. \n",
    "\n",
    "Thankfully, many of the methods that you have come to know and love in numpy are also present in the PyTorch tensor library. When in doubt, I treat things like numpy and usually get 90% there.\n",
    "\n",
    "We'll also make up some fake ratings data for playing with in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Make up some random explicit feedback ratings\n",
    "# and convert to a numpy array\n",
    "n_users = 1000\n",
    "n_items = 1000\n",
    "ratings = sprand(n_users, n_items, \n",
    "                 density=0.01, format='csr')\n",
    "ratings.data = (np.random.randint(1, 5, \n",
    "                                  size=ratings.nnz)\n",
    "                          .astype(np.float64))\n",
    "ratings = ratings.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MatrixFactorization(n_users, n_items, n_factors=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "While PyTorch allows you to define custom loss functions, they thankfully have a default mean squared error loss function in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of different optimization methods in PyTorch, but we'll stick with straight-up Stochastic Gradient Descent for today. One of the fun things about the library is that you are free to choose how to optimize each parameter of your model. Want to have different learning rates for different layers of your neural net? Go for it.\n",
    "\n",
    "We'll keep things simple, though, and use the same technique for all parameters in our model. This requires instantiating an optimization class and passing in the parameters which we want this object to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=1e-6) # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "With the three steps complete, all that's left is to train the model! We have to convert our data into a `Variable` which will talk with the automatic differentiation library, [autograd](http://pytorch.org/docs/autograd.html). To convert the data into a `Variable`, it must first be cast to a torch `Tensor`. Explicitly casting variables is relatively rare in Python, which is probably why I run into the most issues at this stage.\n",
    "\n",
    "You must get your data into Python-native datatypes, `float` and `long`, and numpy likes to make this difficult. For example, you can try to cast a numpy array to `long` by calling `arr.long()`. This is a valid method but does not seem to actually cast the matrix.\n",
    "\n",
    "Nonetheless, below we shuffle our data and iterate through each sample. Each sample is converted to `Variable`, the loss is calculated, backpropagation is carried out to generate the gradients, and then the optimizer updates the parameters. And voil√°! We have matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort our data\n",
    "rows, cols = ratings.nonzero()\n",
    "p = np.random.permutation(len(rows))\n",
    "rows, cols = rows[p], cols[p]\n",
    "\n",
    "for row, col in zip(*(rows, cols)):\n",
    "    # Turn data into variables\n",
    "    rating = Variable(torch.FloatTensor([ratings[row, col]]))\n",
    "    row = Variable(torch.LongTensor([np.long(row)]))\n",
    "    col = Variable(torch.LongTensor([np.long(col)]))\n",
    "    \n",
    "    # Predict and calculate loss\n",
    "    prediction = model(row, col)\n",
    "    loss = loss_func(prediction, rating)\n",
    "    \n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast batched training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpt1,inpt2,labl = [], [], []\n",
    "\n",
    "for row, col in zip(*(rows, cols)):\n",
    "    inpt1.append(np.long(row))\n",
    "    inpt2.append(np.long(col))\n",
    "    labl.append(dat[row, col])\n",
    "\n",
    "inpt1 = Variable(torch.LongTensor(inpt1))\n",
    "inpt2 = Variable(torch.LongTensor(inpt2))\n",
    "labl = Variable(torch.FloatTensor(np.array(labl)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Predict and calculate loss\n",
    "    prediction = model(inpt1, inpt2)\n",
    "    loss = loss_func(prediction, labl)\n",
    "\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "The beauty of the above code is that we can rapidly experiment with different parts of our model and training. For example, adding user and item biases is quite simple via an embedding layer with one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiasedMatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        self.user_biases = torch.nn.Embedding(n_users, \n",
    "                                              1,\n",
    "                                              sparse=True)\n",
    "        self.item_biases = torch.nn.Embedding(n_items,\n",
    "                                              1,\n",
    "                                              sparse=True)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        pred = self.user_biases(user) + self.item_biases(item)\n",
    "        pred += (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{2}$ regularization can easily be added to the entire model via the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_loss_func = torch.optim.SGD(model.parameters(), lr=1e-6,\n",
    "                                weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to get smart about decaying your learning rate, try out a different optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adagrad_loss = torch.optim.Adagrad(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's all just the tip of the iceberg. The real fun is when we stop thinking of recommendation systems as simply user-by-item matrices and incorporate all sorts of side information, context, temporal data, etc... and pass this all through novel network topologies without having to calculate everything by hand. Let me know if you do any of that :)\n",
    "\n",
    "## Caveat\n",
    "\n",
    "To now throw a little water on this optimistic post, it's worth pointing out that there are some costs to abandoning bespoke recommendation systems in favor of repurposing deep learning frameworks. For one, _well-written_ custom solutions are still much faster. Alternating Least Squares is an elegant and more efficient method than SGD for factorizing matrices, and packages like [implicit](https://github.com/benfred/implicit) show this off with their blazingly fast speed. \n",
    "\n",
    "Support for sparse feature learning is still a bit spotty, too. For example, [LightFM](https://github.com/lyst/lightfm) is built from the ground up with sparse features in mind and consequently operates much quicker than a similar solution in a deep learning framework. \n",
    "\n",
    "This post may (read: will) eventually date itself. As GPUs get larger and the frameworks support more sparse operations on the GPU, it may make sense to fully switch over to deep learning frameworks from bespoke CPU solutions.\n",
    "\n",
    "## Further Exploration\n",
    "\n",
    "This post is just a brief introduction to implementing a recommendation system in PyTorch. There are many parts of the code that are poorly optimized. In particular, it is quite helpful to have a generator function/class for loading the data when training. Additionally, one can push computation to a GPU or train in parallel in a [HOGWILD](http://pytorch.org/docs/notes/multiprocessing.html) manner.\n",
    "\n",
    "If you would like to see more, I have started to play around with some explicit and implicit feedback recommendation models Pytorch in a repo [here](https://github.com/ethanrosenthal/torchmf). Maciej Kula, the author of LightFM, also has a repo with recommendation models in Pytorch [here](https://github.com/maciejkula/netrex) (_Update 11/12/2017: Maciej turned that repo into [Spotlight](https://github.com/maciejkula/spotlight) which supports a wide range of recommendation models in PyTorch_).\n",
    "\n",
    "Lastly, why PyTorch for this post? I don't have a great reason other than that I have _really_ enjoyed its general ease of use: it's super easy to install (even with GPU support), it integrates nicely with numpy, and the documentation is fairly extensive. It also feels like there is less noise out there when I am googling for help compared to more popular frameworks like Tensorflow. The downside is the library is a bit rough around the edges, and the API is rapidly changing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
